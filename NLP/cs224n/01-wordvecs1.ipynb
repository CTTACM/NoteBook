{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01-wordvecs1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如何表征单词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.建立词典例如：\n",
    "WordNet:包含同义词集和超限词列表的同义词词典(“是”关系)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun: good\n",
      "noun: good, goodness\n",
      "noun: good, goodness\n",
      "noun: commodity, trade_good, good\n",
      "adj: good\n",
      "adj (s): full, good\n",
      "adj: good\n",
      "adj (s): estimable, good, honorable, respectable\n",
      "adj (s): beneficial, good\n",
      "adj (s): good\n",
      "adj (s): good, just, upright\n",
      "adj (s): adept, expert, good, practiced, proficient, skillful, skilful\n",
      "adj (s): good\n",
      "adj (s): dear, good, near\n",
      "adj (s): dependable, good, safe, secure\n",
      "adj (s): good, right, ripe\n",
      "adj (s): good, well\n",
      "adj (s): effective, good, in_effect, in_force\n",
      "adj (s): good\n",
      "adj (s): good, serious\n",
      "adj (s): good, sound\n",
      "adj (s): good, salutary\n",
      "adj (s): good, honest\n",
      "adj (s): good, undecomposed, unspoiled, unspoilt\n",
      "adj (s): good\n",
      "adv: well, good\n",
      "adv: thoroughly, soundly, good\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "poses = { 'n':'noun', 'v':'verb', 's':'adj (s)', 'a':'adj', 'r':'adv'}\n",
    "for synset in wn.synsets(\"good\"):\n",
    "    print(\"{}: {}\".format(poses[synset.pos()],\", \".join([l.name() for l in synset.lemmas()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('procyonid.n.01'),\n",
       " Synset('carnivore.n.01'),\n",
       " Synset('placental.n.01'),\n",
       " Synset('mammal.n.01'),\n",
       " Synset('vertebrate.n.01'),\n",
       " Synset('chordate.n.01'),\n",
       " Synset('animal.n.01'),\n",
       " Synset('organism.n.01'),\n",
       " Synset('living_thing.n.01'),\n",
       " Synset('whole.n.02'),\n",
       " Synset('object.n.01'),\n",
       " Synset('physical_entity.n.01'),\n",
       " Synset('entity.n.01')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "panda = wn.synset(\"panda.n.01\")\n",
    "hyper = lambda s: s.hypernyms()\n",
    "list(panda.closure(hyper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNet的缺点\n",
    "- 细节缺失\n",
    "- 无法更新\n",
    "- 太过主观\n",
    "- 人工劳动\n",
    "- 无法计算单词的相似度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.用离散符号表示单词\n",
    "例如：one-hot编码：向量的维度等于单词的数量（每个维代表一个单词是否出现）\n",
    "\n",
    "motel = [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
    "\n",
    "hotel = [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one-hot的缺点\n",
    "\n",
    "- 维度太高\n",
    "\n",
    "- 无法计算相似度"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.用语境来表达单词\n",
    "\n",
    "一个词的意思是由那些经常出现在旁边的词给出的。\n",
    "\n",
    "文本中的w,他的**context**是这个w周围出现的词的集合（固定窗口大小）。我们可以用很多不同句子中的**context**去表征这个词w。\n",
    "\n",
    "举例：\n",
    "![1.png](IMAGE/01/1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word vectors\n",
    "\n",
    "我们将为每个单词构建一个密集向量，使其与出现在类似上下文中的单词向量相似。\n",
    "![2.png](IMAGE/01/2.png)\n",
    "\n",
    "**Note*: word vectors有时也被称为word embeddings或者word representations.他是一种distributed representation（分布式表示法）."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 这种表示法的可视化\n",
    "\n",
    "![3.png](IMAGE/01/3.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec: 概述\n",
    "\n",
    "Word2vec是一个学习**词向量(word vectors)**的框架\n",
    "\n",
    "主要想法\n",
    "- 大量的可用文本\n",
    "- 每个单词由一个固定长度的向量来表示\n",
    "- 遍历可用文本的每个位置t，分为**center word（中心词）** c 和 **context words(上下文词) ** o\n",
    "- 利用c和o**词向量**的相似度计算给出c时出现o的概率（或者相反）\n",
    "- 调整**词向量**最大化这个概率\n",
    "\n",
    "如图：\n",
    "\n",
    "step1：\n",
    "\n",
    "![4.png](IMAGE/01/4.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step2：\n",
    "\n",
    "![5.png](IMAGE/01/5.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec: 目标函数\n",
    "文本中的每个位置$t = 1...T.$对固定窗口大小为m的**context words**进行预测，假设**conter word**是$w_j.$\n",
    "\n",
    "$Likelihood = L(\\theta) = \\prod_{t = 1}^{T}\\prod_{-m<=j<=m  j\\neq0}P(w_{t+j}|w_j;\\theta)$\n",
    "\n",
    "![6.png](IMAGE/01/6.png)\n",
    "\n",
    "**Note：$\\theta$*是所需要优化的所有参数 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目标函数（损失函数）$J(\\theta)$是负对数似然的均值：\n",
    "\n",
    "![7.png](IMAGE/01/7.png)\n",
    "最小化目标函数 = 最大化预测正确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问**：如何计算$P(w_{t+j}|w_t;\\theta)$?\n",
    "\n",
    "**答**：\n",
    "每个词$w$用两个向量表示\n",
    "- 当$w$是中心词时用$v_w$表示$w$\n",
    "- 当$w$是上下文词时用$u_w$表示$w$\n",
    "- 然后中心词c和上下文词o有：\n",
    "$\n",
    "P(o|c) = \\frac{exp(u_{o}^{T}v_c)} {\\sum_{w\\in V}exp(u_{w}^{T}v_c)}\n",
    "$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仍然是上方图片中的数据举例：假设$P(u_{problems}|v_{into}) 是P(problems | into;u_{problems},v_{into},\\theta) 的缩写$\n",
    "\n",
    "![8.png](IMAGE/01/8.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中：\n",
    "![9.png](IMAGE/01/9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**疑问**：上式中的$W$和$V$分别指什么？\n",
    "\n",
    "**答**：$w$应该是窗口，$v$应该是整个词库，也就是整个文本"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`softmax function `**函数$ℝ^n → ℝ^n$\n",
    "\n",
    "![10.png](IMAGE/01/10.png)\n",
    "`softmax function `函数把一个任意值映射为一个概率分布：\n",
    "- “max”放大了最大值$x_i$的概率\n",
    "\n",
    "- “soft”仍然将一些概率分配给较小值\n",
    "\n",
    "- 在深度学习中经常使用"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过训练模型优化参数\n",
    "通过训练模型，调整参数最小化损失函数\n",
    "![11.png](IMAGE/01/11.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算所有向量的梯度\n",
    "- $\\theta$ 代表了模型中所有的参数，$\\theta$ 是一个向量\n",
    "\n",
    "- 在我们的案例中,每个单词有两种向量（中心、上下文时各一个），每个向量是d维，一共有V个词汇。\n",
    "\n",
    "- 通过梯度下降法优化参数\n",
    "\n",
    "![12.png](IMAGE/01/12.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度\n",
    "基础知识：\n",
    "- 偏导求解\n",
    "![13.png](IMAGE/01/13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 链式法则，若有$y = f(u)$并且$u = g(x)$,即$y = f(g(x))$,则有：\n",
    "\n",
    "### $\\frac {dy}{dx} = \\frac {dy}{du} \\frac {du}{dx} = \\frac {df(u)}{du} \\frac {dg(x)}{dx}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 例如：\n",
    "\n",
    "$\\frac {dy}{dx} = \\frac {5(x^3+7)^4}{dx}$\n",
    "\n",
    "- $y = f(u) = u^4$\n",
    "\n",
    "- $u = g(x) = x^3 + 7$\n",
    "\n",
    "- $\\frac {dy}{dx} = 20(x^3+7)*3x^2$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算\n",
    "目标函数：\n",
    "![14.png](IMAGE/01/14.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 导出中心词的梯度：\n",
    "![15.png](IMAGE/01/15.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算所有的梯度\n",
    "- 我们计算除了一个窗口中每个中心向量v的梯度\n",
    "\n",
    "- 我们还需要导出的外部向量u的梯度！\n",
    "\n",
    "- 更新该窗口中的参数\n",
    "\n",
    "![16.png](IMAGE/01/16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 更多细节\n",
    "每个单词为什么需要两个向量？---> 易于优化\n",
    "两个变种：\n",
    "- 1. Skip-grams (SG)：给定中心词预测上下文词\n",
    "- 2. Continuous Bag of Words (CBOW)：给定上下文预测中心词\n",
    "\n",
    "我们上面的图片案例用的一直是SG模型\n",
    "\n",
    "效率提升：\n",
    "- 1. 负采样（Negative sampling）\n",
    "\n",
    "上面采用的是朴素的`softmax`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化：梯度下降\n",
    "- 目标函数：$J(\\theta)$\n",
    "- 梯度下降法是一种最优化$J(\\theta)的算法$\n",
    "- 思路：根据当前的参数\\theta,计算$J(\\theta)$的梯度，然后向负梯度方向移动一小步，不断的重复，直到达到优化目的。\n",
    "![17.png](IMAGE/01/17.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 逐步更新\n",
    "![18.png](IMAGE/01/18.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 对于一个参数时：\n",
    "![19.png](IMAGE/01/19.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 算法\n",
    "![20.png](IMAGE/01/20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机梯度下降法\n",
    "问题：$J(\\theta)$是代表所有窗口的函数。\n",
    "- 因此，计算$J(\\theta)$的导数非常的复杂。因此每次更新都要等好久。对于神经网络的运算十分不友好。\n",
    "\n",
    "解决：Stochastic gradient descent (SGD)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 算法：\n",
    "![21.png](IMAGE/01/21.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
